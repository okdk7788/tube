## 로컬 환경에서 한국어 LLM 끝판왕 '사원(Sawon)' 사용하기 튜토리얼

### 서론

안녕하세요! 이 튜토리얼은 LG AI 연구원에서 개발한 오픈소스 AI 모델 '사원(Sawon)'을 여러분의 개인 컴퓨터(로컬 환경)에 설치하고 직접 사용해보는 과정을 안내합니다. 사원은 특히 한국어 능력이 뛰어나다고 알려진 대규모 언어 모델(LLM)입니다.

**강의 목표:**

*   Ollama를 사용하여 사원 모델을 로컬 PC에 설치합니다.
*   Jan(박스)이라는 사용자 인터페이스(UI) 도구를 통해 사원 2.4b, 7.8b, 32b 등 다양한 버전의 모델을 실행하고 비교 체험합니다.
*   사원의 뛰어난 한국어 생성 능력을 직접 확인합니다.

**사원(Sawon)이란?**

*   LG AI 연구원에서 개발한 **오픈소스 AI** 모델입니다.
*   'Exper AI for Everyone'의 약자로, 언어와 이미지를 동시에 이해할 수 있는 멀티모달 능력을 지향합니다.
*   특히 **한국어 구사 능력**이 매우 뛰어난 것으로 평가받습니다.

**핵심 도구:**

*   **Ollama:** 로컬 환경에서 LLM을 쉽게 다운로드하고 실행할 수 있게 도와주는 도구입니다.
*   **Jan (영상에서 '박스'로 언급):** Ollama와 같은 로컬 모델을 위한 사용자 친화적인 데스크탑 인터페이스(UI)입니다.

자, 이제 사원을 로컬 환경에서 만나볼 준비를 시작해 봅시다!

### 본론: 사원 설치 및 사용 단계

#### 사전 준비

1.  **인터넷 연결:** Ollama와 모델 파일을 다운로드하기 위해 필요합니다.
2.  **개인 컴퓨터:** Windows, macOS, Linux 운영체제를 지원합니다. (Ollama 웹사이트에서 시스템 요구사항 확인)
3.  **터미널 또는 명령 프롬프트(CMD):** Ollama 명령어를 입력하기 위해 필요합니다.
    *   macOS/Linux: 터미널(Terminal)
    *   Windows: 명령 프롬프트(CMD) 또는 PowerShell
4.  **(선택 사항) GPU:** 모델 실행 속도를 높이기 위해 그래픽 처리 장치(GPU), 특히 NVIDIA GPU가 있으면 좋습니다. (강의에서는 RTX 4060 사용 언급) CPU만으로도 실행 가능하지만 속도가 느릴 수 있습니다.

#### 1단계: Ollama 설치

1.  웹 브라우저를 열고 구글 등 검색 엔진에서 'Ollama'를 검색합니다.
2.  Ollama 공식 웹사이트([https://ollama.com/](https://ollama.com/))에 접속합니다.
3.  메인 페이지에서 'Download' 버튼을 클릭하고, 사용 중인 운영체제(macOS, Linux, Windows)에 맞는 버전을 다운로드하여 설치합니다. 설치 과정은 일반적인 프로그램 설치와 유사합니다.

#### 2단계: 사원 모델 다운로드 및 실행 (Ollama 사용)

1.  **터미널(macOS/Linux) 또는 명령 프롬프트(CMD - Windows)를 실행**합니다.
2.  Ollama를 사용하여 원하는 사원 모델을 다운로드하고 실행합니다. 강의에서 언급된 7.8B 모델을 예시로 사용해 보겠습니다. 아래 명령어를 터미널/CMD에 입력하고 Enter 키를 누릅니다.

    ```bash
    ollama run sawon:7.8b
    ```

    *   **설명:**
        *   `ollama run`: Ollama를 사용하여 모델을 실행하라는 명령어입니다.
        *   `sawon:7.8b`: 실행할 모델의 이름과 태그(버전)입니다. 'sawon'은 모델 이름, '7.8b'는 7.8 Billion(78억) 파라미터 모델을 의미합니다.
    *   **최초 실행:** 이 명령어를 처음 실행하면 Ollama가 해당 모델 파일을 자동으로 다운로드합니다. 다운로드가 완료되면 모델이 로드되고, 터미널/CMD 창에 `>>>` 와 같은 프롬프트가 나타나며 모델과 대화할 준비가 됩니다.
    *   **다른 버전 실행:** 다른 버전의 사원 모델(예: 2.4b, 32b)을 사용하고 싶다면, 해당 태그로 명령어를 실행하면 됩니다. (사용 가능한 정확한 태그는 Ollama 웹사이트나 `ollama list` 명령어로 확인 가능할 수 있습니다.)
        ```bash
        ollama run sawon:2.4b
        ollama run sawon:32b
        ```
    *   **실행 확인:** 터미널/CMD에서 프롬프트가 나타나면 모델이 성공적으로 실행된 것입니다. 여기서 직접 한국어로 질문하거나 요청을 입력하여 테스트해볼 수 있습니다. (종료하려면 `/bye` 입력)
    *   **주의:** 모델 실행 중에는 터미널/CMD 창을 닫지 마세요. 다음 단계인 Jan에서 사용하려면 Ollama가 백그라운드에서 실행 중이어야 합니다.

#### 3단계: Jan(박스) 설치 및 설정

1.  **Jan 이란?** 영상에서 '박스'라고 언급된 도구는 'Jan'이라는 이름의 데스크탑 애플리케이션일 가능성이 높습니다. Jan은 Ollama 등 로컬 LLM을 위한 편리한 그래픽 사용자 인터페이스(GUI)를 제공합니다.
2.  웹 브라우저에서 'Jan AI'를 검색하여 공식 웹사이트([https://jan.ai/](https://jan.ai/))에 접속합니다.
3.  웹사이트에서 Jan 애플리케이션을 다운로드하여 설치합니다. (OS에 맞게)
4.  Jan을 처음 실행하면 초기 설정 화면이 나타날 수 있습니다.
    *   'Use Local Models', 'Use your own API Key & local models' 또는 이와 유사한 **로컬 모델 사용 옵션을 선택**합니다. (클라우드 구독 옵션이 아닌 로컬 사용 옵션을 선택해야 합니다.)
    *   'Server Provider', 'Engine', 또는 유사한 설정 항목에서 **'Ollama API'를 선택**합니다. Jan이 로컬에서 실행 중인 Ollama와 통신하도록 설정하는 것입니다.

#### 4단계: Jan(박스)에서 사원 모델 사용 및 테스트

1.  **Ollama 실행 확인:** Jan을 사용하기 전에 **Ollama가 백그라운드에서 실행 중**이어야 합니다. (보통 시스템 트레이 아이콘 등으로 확인 가능) 만약 실행 중이 아니라면, 터미널/CMD에서 `ollama serve` 명령어를 실행하여 백그라운드 서버를 띄울 수 있습니다.
2.  **Jan 인터페이스에서 모델 선택:**
    *   Jan 인터페이스의 하단이나 설정 메뉴 등에서 모델을 선택하는 부분을 찾습니다.
    *   'Ollama API'를 통해 접근 가능한 모델 목록이 나타납니다. 2단계에서 `ollama run` 명령어로 다운로드/실행했던 사원 모델들(예: `sawon:2.4b`, `sawon:7.8b`, `sawon:32b`)이 보여야 합니다.
    *   테스트하고 싶은 사원 모델 버전을 선택합니다. (예: `sawon:7.8b`)
3.  **채팅 및 테스트:**
    *   이제 Jan의 채팅 입력창에 한국어로 프롬프트를 입력하여 사원 모델과 대화를 시작할 수 있습니다.
    *   **강의에서 사용된 예시 프롬프트:**
        *   "청년들에게 희망을 주는 단편 소설을 써 주세요."
        *   "환율에 관한 보고서를 작성해 주세요."
        *   "우주에 관한 장편 소설을 써 주세요." (원 영상에서는 단편 소설 요청)
        *   "무역에 대한 보고서를 써 주세요."
        *   "바다에 대한 단편 소설을 써 주세요."
    *   모델의 답변을 확인하고 한국어 구사 능력, 내용의 자연스러움 등을 평가해 보세요.
4.  **모델 변경 및 비교:**
    *   Jan 인터페이스에서 다른 사원 모델 버전(예: `sawon:2.4b`, `sawon:32b`)으로 쉽게 전환할 수 있습니다.
    *   같은 프롬프트를 다른 모델 버전에게 입력하여 응답 속도와 결과물의 차이를 비교해 보세요.
    *   **강의 내용 요약:**
        *   **속도:** 일반적으로 모델 크기가 작을수록(2.4b < 7.8b < 32b) 응답 속도가 빠릅니다. (사용자 PC의 GPU 성능에 크게 영향받음)
        *   **품질:** 일반적으로 모델 크기가 클수록 더 정교하고 품질 높은 결과물을 생성하는 경향이 있습니다. 하지만 32b 모델은 응답 속도가 상당히 느릴 수 있습니다.
        *   **활용 팁:** 사원은 현재(영상 시점 기준) 실시간 웹 검색 기능이 없으므로 최신 정보나 사실 확인보다는, 주어진 예시나 특정 형식에 맞춰 글을 생성하거나(예: 보고서, 소설), 한국어 텍스트를 다듬는 등의 작업에 더 강점을 보일 수 있습니다.

### 결론

이 튜토리얼을 통해 여러분은 성공적으로 다음을 수행했습니다:

*   Ollama를 사용하여 강력한 한국어 LLM '사원'을 로컬 PC에 설치했습니다.
*   Jan(박스)이라는 편리한 UI 도구를 통해 사원 2.4b, 7.8b, 32b 모델을 직접 실행하고 테스트했습니다.
*   각 모델 버전의 응답 속도와 한국어 생성 능력의 차이를 경험했습니다.

**최종 결과물:** 여러분의 PC에서 직접 구동되는 한국어 AI 비서 '사원'을 갖게 되었습니다!

사원은 뛰어난 한국어 능력을 바탕으로 다양한 글쓰기, 요약, 아이디어 구상 등에 활용될 잠재력이 큽니다. 다만, 모델 크기와 사용자의 PC 성능(특히 GPU 유무)에 따라 사용 경험(속도, 품질)이 달라질 수 있다는 점을 기억하세요.

강의에서 언급되었듯이, 오픈소스 AI는 잘 활용할 수 있다면 매우 가치 있는 도구가 될 수 있습니다. 이 튜토리얼이 여러분의 AI 여정에 도움이 되었기를 바랍니다!

### 용어 설명

*   **LLM (Large Language Model):** 대규모 언어 모델. 방대한 텍스트 데이터를 학습하여 인간의 언어를 이해하고 생성할 수 있는 인공지능 모델입니다. (예: 사원, GPT-3/4, 라마)
*   **오픈소스 AI (Open Source AI):** 소스 코드나 모델 가중치 등이 공개되어 누구나 자유롭게 사용, 연구, 수정, 배포할 수 있는 AI 기술 또는 모델을 의미합니다.
*   **로컬 환경 (Local Environment):** 인터넷상의 원격 서버(클라우드)가 아닌, 사용자 개인의 컴퓨터(PC, 노트북) 환경을 의미합니다.
*   **API (Application Programming Interface):** 응용 프로그램 프로그래밍 인터페이스. 프로그램이나 서비스가 서로 상호작용(통신)하기 위한 규격 또는 방법을 의미합니다. Jan이 Ollama와 통신할 때 Ollama API를 사용합니다.
*   **매개변수 (Parameter, 'b'):** 모델의 복잡성과 크기를 나타내는 지표 중 하나입니다. 수십억(Billion, 'b') 단위로 표현되는 경우가 많으며, 일반적으로 파라미터 수가 많을수록 모델의 성능이 높지만 더 많은 계산 능력(메모리, 연산 속도)을 요구합니다.